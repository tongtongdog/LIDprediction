{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1e64e3-4d55-4006-9b3b-ec336cf0be73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import glob\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=Warning)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy import stats\n",
    "\n",
    "import pandas as pd\n",
    "import xlsxwriter\n",
    "import joblib\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, auc, f1_score\n",
    "\n",
    "import xgboost as xgb\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9268518-1fae-4a25-bb92-cb590b2e10a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## for binarizing sex variable\n",
    "def getTF(x):\n",
    "    if x=='M': return 0.0\n",
    "    elif x=='F': return 1.0\n",
    "    else: raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f58ced-f39f-4106-a551-78b7b38c1f59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_cli = pd.read_excel('/workspace/list_of_patients_with_clinical_information.xlsx', sheet_name=0)\n",
    "\n",
    "df_cli = df_cli[['PET_ID', 'Sex', 'Symptom_Onset_Age', 'Symptom_Onset_Age_60', 'Gap_in_3months', 'HY_rnd', \n",
    "                 'RT_RA', 'RT_LA', 'RT_RL', 'RT_LL', 'FT_R', 'FT_L', 'LA_R', 'LA_L', 'RG_RA', 'RG_LA',\n",
    "                 '1_VS/Calc_Bi-1', '2_PP/Calc_Bi-1', '3_PC/Calc_Bi-1', '4_AC/Calc_Bi-1', '5_AP/Calc_Bi-1', \n",
    "                 '1_Lim/Calc_Bi-1', '2_Exe/Calc_Bi-1', '3_Sen/Calc_Bi-1', 'Whole_putamen', 'LID_within_5Y', 'LEDD', 'Cumulative_LED']]\n",
    "\n",
    "df_with_na = df_cli[df_cli.isna().any(axis=1)]\n",
    "\n",
    "target_list = ['HY_rnd', 'RT_RA', 'RT_LA', 'RT_RL', 'RT_LL', 'FT_R', 'FT_L', 'LA_R', 'LA_L', 'RG_RA', 'RG_LA']\n",
    "\n",
    "## impute missing values with the median value\n",
    "for var in target_list:\n",
    "    df_cli[var] = df_cli[var].fillna(df_cli[var].median())\n",
    "\n",
    "df_cli_filled_na = df_cli.iloc[df_with_na.index]\n",
    "df_cli_filled_na = df_cli_filled_na[target_list] \n",
    "\n",
    "\n",
    "df_cli['Tremor_total'] = df_cli['RT_RA'] + df_cli['RT_LA'] + df_cli['RT_RL'] + df_cli['RT_LL']\n",
    "df_cli['Brady_total'] = df_cli['FT_R'] + df_cli['FT_L'] + df_cli['LA_R'] + df_cli['LA_L']\n",
    "df_cli['Rigid_total'] = df_cli['RG_RA'] + df_cli['RG_LA']\n",
    "\n",
    "df_cli['RT_UEx'] = df_cli['RT_RA'] + df_cli['RT_LA']\n",
    "df_cli['RT_LEx'] = df_cli['RT_RL'] + df_cli['RT_LL']\n",
    "\n",
    "df_cli['FT'] = df_cli['FT_R'] + df_cli['FT_L']\n",
    "df_cli['LA'] = df_cli['LA_R'] + df_cli['LA_L']\n",
    "\n",
    "df_cli['RG_UEx'] = df_cli['RG_RA'] + df_cli['RG_LA']\n",
    "\n",
    "\n",
    "cont_list = ['Symptom_Onset_Age', 'Symptom_Onset_Age_60', 'Gap_in_3months', 'HY_rnd', 'Tremor_total', 'Brady_total', 'Rigid_total',\n",
    "             '1_VS/Calc_Bi-1', '2_PP/Calc_Bi-1', '3_PC/Calc_Bi-1', '4_AC/Calc_Bi-1', '5_AP/Calc_Bi-1', \n",
    "             '1_Lim/Calc_Bi-1', '2_Exe/Calc_Bi-1', '3_Sen/Calc_Bi-1', 'Whole_putamen', 'LEDD', 'Cumulative_LED',\n",
    "             'RT_UEx', 'RT_LEx', 'FT', 'LA', 'RG_UEx']\n",
    "\n",
    "up_list = ['RT_RA', 'RT_LA', 'RT_RL', 'RT_LL', 'FT_R', 'FT_L', 'LA_R', 'LA_L', 'RG_RA', 'RG_LA']\n",
    "cat_list = ['Sex']\n",
    "id_list = ['PET_ID']\n",
    "\n",
    "df_cli = df_cli.copy()\n",
    "\n",
    "## sex to either 0 or 1\n",
    "df_cli.loc[:,'Sex_Z'] = df_cli[\"Sex\"].map(lambda x:getTF(x))\n",
    "\n",
    "## updrs 0~4 --> 0~2\n",
    "for var in up_list:\n",
    "    new_col = var+'_Z'\n",
    "    # df_cli_338.loc[:, new_col] = df_cli_338[var].map(lambda x:x/4)\n",
    "    df_cli.loc[:, new_col] = df_cli[var].map(lambda x:x/2)\n",
    "    \n",
    "scaler = MinMaxScaler(feature_range=(0,2))\n",
    "for var in cont_list:\n",
    "    new_col = var+'_Z'\n",
    "    df_cli.loc[:, new_col] = scaler.fit_transform(df_cli[[var]])\n",
    "\n",
    "\n",
    "## drop the original columns and keep the normalized columns\n",
    "df_z = df_cli.drop(['Sex', 'Symptom_Onset_Age', 'Gap_in_3months', 'HY_rnd', 'RT_RA', 'RT_LA', 'RT_RL', 'RT_LL', 'FT_R', 'FT_L', 'LA_R', 'LA_L', 'RG_RA', 'RG_LA', \n",
    "                    'Symptom_Onset_Age_60', 'Tremor_total', 'Brady_total', 'Rigid_total', 'RT_UEx', 'RT_LEx', 'FT', 'LA', 'RG_UEx',\n",
    "                    '1_VS/Calc_Bi-1', '2_PP/Calc_Bi-1', '3_PC/Calc_Bi-1', '4_AC/Calc_Bi-1', '5_AP/Calc_Bi-1', \n",
    "                    '1_Lim/Calc_Bi-1', '2_Exe/Calc_Bi-1', '3_Sen/Calc_Bi-1', 'Whole_putamen', 'LEDD', 'Cumulative_LED'], axis=1)\n",
    "display(df_z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5278ad8-867b-44d7-8831-d18730fc8d0a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Machine learning models: LR, RF, XGBoost\n",
    "\n",
    "## Variables: SNBRs only, SNBR + clinical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9625e3-d064-490d-8551-0061ffb6a659",
   "metadata": {},
   "source": [
    "### - utils for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec48424-904a-40ce-872c-45bfd8b70800",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_accuracy(y_truth, y_pred, class_names, title='Confusion matrix', fold_num=0, SAVE_FOLDER='/workspace/folder_to_save_results/'):\n",
    "    num_TN = 0\n",
    "    num_FP = 0\n",
    "    num_TP = 0\n",
    "    num_FN = 0\n",
    "    \n",
    "    for idx in range(0, len(y_truth)):\n",
    "        if y_truth[idx] == 0:\n",
    "            if y_pred[idx] == 0:\n",
    "                num_TN += 1\n",
    "            else:\n",
    "                num_FP += 1\n",
    "        else:\n",
    "            if y_pred[idx] == 1:\n",
    "                num_TP += 1\n",
    "            else:\n",
    "                num_FN += 1\n",
    "    print ('       TN =', num_TN, '/ FP =', num_FP)\n",
    "    print ('       FN =', num_FN, '/ TP =', num_TP)\n",
    "    \n",
    "    sen = float(num_TP) / float(num_TP + num_FN)\n",
    "    print ('       sensitivity:', round(sen*100, 2), '%')\n",
    "    \n",
    "    spec = float(num_TN) / float(num_TN + num_FP)\n",
    "    print ('       specificity:', round(spec*100, 2), '%')\n",
    "    \n",
    "    test_acc = accuracy_score(y_truth, y_pred)\n",
    "    print ('       Accuracy >> ' + str(round(test_acc * 100, 2)) + '%\\n')\n",
    "    \n",
    "    plot_confusion_matrix(confusion_matrix(y_truth, y_pred), classes=class_names, title=title, fold_num=fold_num, SAVE_FOLDER=SAVE_FOLDER)\n",
    "    \n",
    "    return sen, spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d8af9d-146f-4377-a22b-2d2b8c55aeca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes, normalize=True, title='Confusion matrix', cmap=plt.cm.Blues, fold_num=0, SAVE_FOLDER='/workspace/codes/folder_to_save_results/'):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    \n",
    "    if normalize:\n",
    "        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix')\n",
    "\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title, fontsize='x-large')\n",
    "    plt.colorbar(pad=0.02)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, fontsize='large')\n",
    "    plt.yticks(tick_marks, classes, fontsize='large', rotation=90)\n",
    "\n",
    "   \n",
    "    thresh = cm.max() / 2.\n",
    "    \n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize and cm_normalized is not None:\n",
    "            text = f\"{cm[i, j]}\\n({cm_normalized[i, j]:.1f}%)\"\n",
    "        else:\n",
    "            text = f\"{cm[i, j]}\"\n",
    "        \n",
    "        plt.text(j, i, text,\n",
    "                 horizontalalignment=\"center\",\n",
    "                 verticalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\", size='x-large')\n",
    "\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label', fontsize='x-large')\n",
    "    plt.yticks(tick_marks, ['LID-', 'LID+'], va='center', fontsize='x-large')\n",
    "    plt.xlabel('Predicted label', fontsize='x-large')\n",
    "\n",
    "    plt.savefig(SAVE_FOLDER + f'/Confusion_matrix_{title}.png', dpi=300, bbox_inches='tight') \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03e5445-33ef-420f-b49f-55f09535a986",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_auroc_3(tpr, fpr, data, currrent_ml, SAVE_FOLDER):\n",
    "\n",
    "    plt.figure(figsize=(6,6), dpi=150)\n",
    "    \n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, lw=1.5, alpha=0.8, color='b', linestyle='solid', label='%s, AUC=%0.3f' % (data, round(roc_auc,3)))\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='grey', alpha=.8)\n",
    "\n",
    "   \n",
    "    plt.xlim([-0.01, 1.01])\n",
    "    plt.ylim([-0.01, 1.01])\n",
    "    plt.xlabel('1 - Specificity', fontsize = 12)\n",
    "    plt.ylabel('Sensitivity', fontsize = 12)\n",
    "    plt.title(currrent_ml, fontsize = 14)\n",
    "    plt.legend(loc=\"lower right\", fontsize = 12)\n",
    "    \n",
    "    plt.gcf().patch.set_alpha(1.0)\n",
    "    plt.savefig(SAVE_FOLDER + f'/ROC_{currrent_ml}.png', dpi=300, bbox_inches='tight') \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4488a4bb-015a-423f-a9cd-9026b8a0d2b6",
   "metadata": {},
   "source": [
    "### - LR, RF, and XGBoost functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7c4143-f810-4fd4-bf96-d74b95aee417",
   "metadata": {
    "tags": []
   },
   "source": [
    "-  (df) X_train: row is for the subjects, column is for the variables\n",
    "-  (df) y_train: 0 or 1\n",
    "-  (df) X_test\n",
    "-  (df) y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688f8d25-ecd3-4e5f-930e-0ec3fbe28d1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def logistic_regression_run_2(X_train, y_train, X_test, y_test, skf, my_model, SAVE_FOLDER):\n",
    "    \n",
    "    if not os.path.exists(SAVE_FOLDER):\n",
    "        os.makedirs(SAVE_FOLDER)\n",
    "        \n",
    " \n",
    "    n_features = []\n",
    "    fpr_list, tpr_list, thr_list, data_list = [], [], [], []\n",
    "    sen, spec, sen_roc, spec_roc, accuracy, f1_scores, aurocs = [], [], [], [], [], [], []\n",
    "    \n",
    "    y_prob_test_all_folds = []\n",
    "    \n",
    "    cur_fold = 0\n",
    "\n",
    "    X_test = np.asarray(X_test, dtype=\"float32\")\n",
    "    y_test = np.asarray(y_test)\n",
    "\n",
    "    \n",
    "    ######## LR ##############################################\n",
    "    print (\"[logistic regression]\")\n",
    "    clsf_LR = LogisticRegression()\n",
    "    \n",
    "    print (\"grid search ......\")\n",
    "    grid = {\"class_weight\": [\"balanced\", None], \"C\": np.logspace(-3, 3, 7), \"penalty\": [\"l1\",\"l2\",\"elasticnet\"], \"class_weight\": [\"balanced\"]}\n",
    "    gs = GridSearchCV(clsf_LR, grid, scoring='roc_auc', cv=skf)\n",
    "\n",
    "    print (\"fit ......\")\n",
    "    gs = gs.fit(X_train, np.ravel(y_train, order='C'))\n",
    "    print (\"Hyperparameter: \", gs.best_params_)\n",
    "    \n",
    "    clsf_LR = LogisticRegression(class_weight = gs.best_params_['class_weight'], C = gs.best_params_['C'], penalty = gs.best_params_['penalty'])\n",
    "    clsf_LR = clsf_LR.fit(X_train, np.ravel(y_train, order='C'))\n",
    "\n",
    "\n",
    "    ######### Calculate evaluation metrics ###########################\n",
    "    y_prob_test_set = clsf_LR.predict_proba(X_test)[:,clsf_LR.classes_[1]]    ## for the test set\n",
    "\n",
    "    fpr, tpr, th = roc_curve(y_test, y_prob_test_set)\n",
    "    \n",
    "    youden = np.argmax(tpr-fpr)\n",
    "    print (\"Youden index:\", th[youden])\n",
    "   \n",
    "    final_predictions = (y_prob_test_set >= th[youden]).astype(bool) \n",
    "    \n",
    "    print (\"Label: \", y_test)\n",
    "    print (\"Prediction: \", final_predictions)\n",
    "    print (\"Threshold: \", th[youden])\n",
    "    \n",
    "    _accuracy = accuracy_score(y_test, final_predictions)\n",
    "    print ('accuracy: ', str(_accuracy*100) + '%')\n",
    "    \n",
    "    _f1_score = f1_score(y_test, final_predictions)\n",
    "    print ('f1 score: ', _f1_score)\n",
    "    \n",
    "    _sen, _spec = calculate_accuracy(y_test, final_predictions, [\"LID-\", \"LID+\"], f'{my_model}_LR_test', 0, SAVE_FOLDER)\n",
    "    print ('Sensitivity: ', _sen)\n",
    "    print ('Specificity: ', _spec)\n",
    "\n",
    "    _auroc = auc(fpr, tpr)\n",
    "    \n",
    "    #================================================================================\n",
    "\n",
    "    plot_auroc_3(tpr, fpr, 'Test set', f'{my_model}_LR', SAVE_FOLDER)\n",
    "\n",
    "    df_test_results = pd.DataFrame({'Model': [f'({my_model})_LR'], 'Fold': ['test'], \n",
    "                            'Accuracy': _accuracy, 'Sensitivity': _sen, 'Specificity': _spec, 'F1 score': _f1_score, 'AUROC': _auroc})\n",
    "\n",
    "\n",
    "    return y_test, y_prob_test_set, df_test_results, clsf_LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e464e4-0d7b-43ea-baa7-1ee1695b3d76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rf_run_2(X_train, y_train, X_test, y_test, skf, my_model, SAVE_FOLDER, seed):\n",
    "    \n",
    "    if not os.path.exists(SAVE_FOLDER):\n",
    "        os.makedirs(SAVE_FOLDER)\n",
    "        \n",
    " \n",
    "    n_features = []\n",
    "    fpr_list, tpr_list, thr_list, data_list = [], [], [], []\n",
    "    sen, spec, sen_roc, spec_roc, accuracy, f1_scores, aurocs = [], [], [], [], [], [], []\n",
    "    \n",
    "    y_prob_test_all_folds = []\n",
    "    \n",
    "    cur_fold = 0\n",
    "\n",
    "    X_test = np.asarray(X_test, dtype=\"float32\")\n",
    "    y_test = np.asarray(y_test)\n",
    "\n",
    "    \n",
    "    ######## LR ##############################################\n",
    "    print (\"[random forest]\")\n",
    "    clsf_RF = RandomForestClassifier(random_state=seed)\n",
    "\n",
    "    print (\"grid search ......\")\n",
    "    grid = { 'random_state': [999], 'class_weight': ['balanced'], 'criterion': ['gini'], 'n_estimators': [100, 200, 300],\n",
    "        'min_samples_split': [2, 4, 8], 'max_features': [1, 3], 'max_depth': [2, 4, 6] }\n",
    "    gs = GridSearchCV(clsf_RF, grid, scoring='roc_auc', cv=skf)\n",
    "    \n",
    "\n",
    "    print (\"fit ......\")\n",
    "    gs = gs.fit(X_train, np.ravel(y_train, order='C'))\n",
    "    print (\"Hyperparameter: \", gs.best_params_)\n",
    "\n",
    "    clsf_RF = RandomForestClassifier(random_state = 999, class_weight = gs.best_params_['class_weight'], criterion = gs.best_params_['criterion'], \n",
    "                                     n_estimators = gs.best_params_['n_estimators'], min_samples_split = gs.best_params_['min_samples_split'],\n",
    "                                     max_features = gs.best_params_['max_features'], max_depth = gs.best_params_['max_depth'])\n",
    "    clsf_RF = clsf_RF.fit(X_train, np.ravel(y_train, order='C'))\n",
    "\n",
    "\n",
    "    ######### Calculate evaluation metrics ###########################\n",
    "    y_prob_test_set = clsf_RF.predict_proba(X_test)[:,clsf_RF.classes_[1]]    ## for the test set\n",
    "\n",
    "    fpr, tpr, th = roc_curve(y_test, y_prob_test_set)\n",
    "    \n",
    "    youden = np.argmax(tpr-fpr)\n",
    "    print (\"Youden index:\", th[youden])\n",
    "   \n",
    "    final_predictions = (y_prob_test_set >= th[youden]).astype(bool) \n",
    "    \n",
    "    print (\"Label: \", y_test)\n",
    "    print (\"Prediction: \", final_predictions)\n",
    "    print (\"Threshold: \", th[youden])\n",
    "    \n",
    "    _accuracy = accuracy_score(y_test, final_predictions)\n",
    "    print ('accuracy: ', str(_accuracy*100) + '%')\n",
    "    \n",
    "    _f1_score = f1_score(y_test, final_predictions)\n",
    "    print ('f1 score: ', _f1_score)\n",
    "    \n",
    "    _sen, _spec = calculate_accuracy(y_test, final_predictions, [\"LID-\", \"LID+\"], f'{my_model}_RF_test', 0, SAVE_FOLDER)\n",
    "    print ('Sensitivity: ', _sen)\n",
    "    print ('Specificity: ', _spec)\n",
    "\n",
    "    _auroc = auc(fpr, tpr)\n",
    "    \n",
    "    #================================================================================\n",
    "\n",
    "    plot_auroc_3(tpr, fpr, 'Test set', f'{my_model}_RF', SAVE_FOLDER)\n",
    "\n",
    "    df_test_results = pd.DataFrame({'Model': [f'({my_model})_RF'], 'Fold': ['test'], \n",
    "                            'Accuracy': _accuracy, 'Sensitivity': _sen, 'Specificity': _spec, 'F1 score': _f1_score, 'AUROC': _auroc})\n",
    "\n",
    "\n",
    "    return y_test, y_prob_test_set, df_test_results, clsf_RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5065fa7-e927-4c3d-9720-b1340c812820",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def xgboost_run_2(X_train, y_train, X_test, y_test, skf, my_model, SAVE_FOLDER, seed):\n",
    "    \n",
    "    if not os.path.exists(SAVE_FOLDER):\n",
    "        os.makedirs(SAVE_FOLDER)\n",
    "        \n",
    " \n",
    "    n_features = []\n",
    "    fpr_list, tpr_list, thr_list, data_list = [], [], [], []\n",
    "    sen, spec, sen_roc, spec_roc, accuracy, f1_scores, aurocs = [], [], [], [], [], [], []\n",
    "    \n",
    "    y_prob_test_all_folds = []\n",
    "    \n",
    "    cur_fold = 0\n",
    "\n",
    "    X_test = np.asarray(X_test, dtype=\"float32\")\n",
    "    y_test = np.asarray(y_test)\n",
    "\n",
    "    \n",
    "    ######## LR ##############################################\n",
    "    print (\"[XGBoost]\")\n",
    "    clsf_xgb = xgb.XGBClassifier(verbosity = 0, random_state=seed)\n",
    "\n",
    "    print (\"grid search ......\")\n",
    "    grid = { 'random_state': [999], 'objective':['binary:logistic'], 'max_depth': [1, 3],\n",
    "        'subsample': [0.4, 0.6, 0.8], 'colsample_bytree': [0.1, 0.3, 0.5], 'n_estimators': [300, 500, 700, 1000] }\n",
    "\n",
    "    gs = GridSearchCV(clsf_xgb, grid, scoring='roc_auc', cv=skf)\n",
    "    \n",
    "    print (\"fit ......\")\n",
    "    gs = gs.fit(X_train, np.ravel(y_train, order='C'))\n",
    "    print (\"Hyperparameter: \", gs.best_params_)\n",
    "\n",
    "    clsf_xgb = xgb.XGBClassifier(random_state = 999, objective = 'binary:logistic', max_depth = gs.best_params_['max_depth'], \n",
    "                                 subsample = gs.best_params_['subsample'], colsample_bytree = gs.best_params_['colsample_bytree'],\n",
    "                                 n_estimators = gs.best_params_['n_estimators'])\n",
    "    clsf_xgb = clsf_xgb.fit(X_train, np.ravel(y_train, order='C'))\n",
    "\n",
    "\n",
    "    ######### Calculate evaluation metrics ###########################\n",
    "    y_prob_test_set = clsf_xgb.predict_proba(X_test)[:,clsf_xgb.classes_[1]]    ## for the test set\n",
    "\n",
    "    fpr, tpr, th = roc_curve(y_test, y_prob_test_set)\n",
    "    \n",
    "    youden = np.argmax(tpr-fpr)\n",
    "    print (\"Youden index:\", th[youden])\n",
    "   \n",
    "    final_predictions = (y_prob_test_set >= th[youden]).astype(bool) \n",
    "    \n",
    "    print (\"Label: \", y_test)\n",
    "    print (\"Prediction: \", final_predictions)\n",
    "    print (\"Threshold: \", th[youden])\n",
    "    \n",
    "    _accuracy = accuracy_score(y_test, final_predictions)\n",
    "    print ('accuracy: ', str(_accuracy*100) + '%')\n",
    "    \n",
    "    _f1_score = f1_score(y_test, final_predictions)\n",
    "    print ('f1 score: ', _f1_score)\n",
    "    \n",
    "    _sen, _spec = calculate_accuracy(y_test, final_predictions, [\"LID-\", \"LID+\"], f'{my_model}_XGB_test', 0, SAVE_FOLDER)\n",
    "    print ('Sensitivity: ', _sen)\n",
    "    print ('Specificity: ', _spec)\n",
    "\n",
    "    _auroc = auc(fpr, tpr)\n",
    "    \n",
    "    #================================================================================\n",
    "\n",
    "    plot_auroc_3(tpr, fpr, 'Test set', f'{my_model}_XGB', SAVE_FOLDER)\n",
    "\n",
    "    df_test_results = pd.DataFrame({'Model': [f'({my_model})_XGB'], 'Fold': ['test'], \n",
    "                            'Accuracy': _accuracy, 'Sensitivity': _sen, 'Specificity': _spec, 'F1 score': _f1_score, 'AUROC': _auroc})\n",
    "\n",
    "\n",
    "    return y_test, y_prob_test_set, df_test_results, clsf_xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece56788-a8a0-4e6e-b921-a06974aa3192",
   "metadata": {},
   "source": [
    "### - SHAP function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b7cde8-3f4b-446c-bf4d-7fca5f078088",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_shap (X_test, shap_values, my_model, ml_method, SAVE_FOLDER, current_seed):      ## X_train, y_train are dataframes\n",
    "\n",
    "    ## beeswarm plot\n",
    "    shap.summary_plot(shap_values, X_test, show=False, max_display=10)\n",
    "    plt.xlabel(\"SHAP value\")\n",
    "    plt.title(f'{my_model} {ml_method} (Test set)', fontsize=14)\n",
    "    plt.savefig(SAVE_FOLDER + f'/Shapley {my_model} {ml_method} s{current_seed} bee.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # bar plot\n",
    "    plt.figure()\n",
    "    shap.summary_plot(shap_values, X_test, plot_type=\"bar\", show=False, max_display=10) \n",
    "    plt.xlabel(\"mean(|SHAP value|)\")\n",
    "    plt.title(f'{my_model} {ml_method} (Test set)', fontsize=14)\n",
    "    for bar in plt.gca().patches:\n",
    "        bar.set_facecolor(\"red\")\n",
    "    plt.savefig(SAVE_FOLDER + f'/Shapley {my_model} {ml_method} s{current_seed} bar.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513c3e89-024f-4150-945c-1fac57eb7803",
   "metadata": {},
   "source": [
    "### - Code for running the ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6f45b4-eaba-4395-b08b-22bee1cf9921",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## need to use the same seeds used to split the data for training the deep learning model (from notebook: preparation_for_cross_validation.ipynb)\n",
    "random_seed = [768, 187, 488, 758, 915]\n",
    "fold_list = ['Fold_0', 'Fold_1', 'Fold_2', 'Fold_3', 'Fold_4']\n",
    "\n",
    "target_path = '/workspace/data_folder'\n",
    "SAVE_FOLDER = '/workspace/save_results_folder'\n",
    "\n",
    "model_list = ['snbr', 'snbr + clinical_vars']\n",
    "\n",
    "df_finale_results = pd.DataFrame()\n",
    "\n",
    "for my_model in model_list:\n",
    "    \n",
    "    for current_fold, current_seed in zip(fold_list, random_seed):\n",
    "\n",
    "        current_folder = os.path.join(target_path, current_fold)\n",
    "        \n",
    "        ## choose the right columns for each model\n",
    "        df_current = df_z\n",
    "        \n",
    "        if my_model=='snbr':\n",
    "            df_current = df_current[['PET_ID', 'LID_within_5Y', '1_VS/Calc_Bi-1_Z', '2_PP/Calc_Bi-1_Z', '3_PC/Calc_Bi-1_Z', '4_AC/Calc_Bi-1_Z', '5_AP/Calc_Bi-1_Z', \n",
    "                                     '1_Lim/Calc_Bi-1_Z', '2_Exe/Calc_Bi-1_Z', '3_Sen/Calc_Bi-1_Z']]\n",
    "            \n",
    "        elif my_model=='snbr + clinical_vars':\n",
    "            df_current = df_current[['PET_ID', 'LID_within_5Y', 'Sex_Z', 'Symptom_Onset_Age_Z', 'Gap_in_3months_Z',\n",
    "                                     'HY_rnd_Z', 'RT_UEx_Z', 'RT_LEx_Z', 'FT_Z', 'LA_Z', 'RG_UEx_Z',\n",
    "                                     '1_VS/Calc_Bi-1_Z', '2_PP/Calc_Bi-1_Z', '3_PC/Calc_Bi-1_Z', '4_AC/Calc_Bi-1_Z', '5_AP/Calc_Bi-1_Z']]      \n",
    "        else:\n",
    "            print('Wrong model name!')\n",
    "            raise\n",
    "\n",
    "        \n",
    "        ## X_train, y_train, X_test, y_test\n",
    "        train_list = glob.glob(current_folder+'/train/*_img.nii.gz')\n",
    "        train_list = [os.path.basename(x).split('_')[0] + '_' + os.path.basename(x).split('_')[1] for x in train_list]\n",
    "\n",
    "        test_list = glob.glob(current_folder+'/test/*_img.nii.gz')\n",
    "        test_list = [os.path.basename(x).split('_')[0] + '_' + os.path.basename(x).split('_')[1] for x in test_list]\n",
    "\n",
    "        X_train = df_current[df_current['PET_ID'].isin(train_list)].drop(['PET_ID', 'LID_within_5Y'], axis=1)\n",
    "        y_train = df_current[df_current['PET_ID'].isin(train_list)]['LID_within_5Y']\n",
    "\n",
    "        X_test = df_current[df_current['PET_ID'].isin(test_list)].drop(['PET_ID', 'LID_within_5Y'], axis=1)\n",
    "        y_test = df_current[df_current['PET_ID'].isin(test_list)]['LID_within_5Y']\n",
    "\n",
    "\n",
    "        original_path = os.path.join(target_path, current_fold, 'train')\n",
    "        total_list = sorted(os.listdir(original_path))\n",
    "        total_list = total_list[0::2]\n",
    "\n",
    "        X = [s[:-11] for s in total_list]\n",
    "        Y = []\n",
    "\n",
    "        for i in total_list:\n",
    "            if '_no_' in i: \n",
    "                Y.append(0)\n",
    "            elif '_yes_' in i: \n",
    "                Y.append(1)\n",
    "            else:\n",
    "                print('I am a wanderer!')\n",
    "\n",
    "        skf = StratifiedKFold(n_splits=5, random_state=current_seed, shuffle=True)\n",
    "\n",
    "        ## logistic regression\n",
    "        y_test_LR, y_prob_test_set_LR, df_test_results_LR, clsf_LR = logistic_regression_run_2(X_train, y_train, X_test, y_test, skf, f'{my_model}_s{current_seed}', SAVE_FOLDER)\n",
    "        y_test_RF, y_prob_test_set_RF, df_test_results_RF, clsf_RF = rf_run_2(X_train, y_train, X_test, y_test, skf, f'{my_model}_s{current_seed}', SAVE_FOLDER, current_seed)\n",
    "        y_test_XGB, y_prob_test_set_XGB, df_test_results_XGB, clsf_XGB = xgboost_run_2(X_train, y_train, X_test, y_test, skf, f'{my_model}_s{current_seed}', SAVE_FOLDER, current_seed)\n",
    "        \n",
    "        df_finale_results = pd.concat([df_finale_results, df_test_results_LR, df_test_results_RF, df_test_results_XGB], axis=0)\n",
    "        \n",
    "        \n",
    "        # Save models\n",
    "        model_path_LR = os.path.join(SAVE_FOLDER, f's{current_seed}_LR_model.joblib')\n",
    "        model_path_RF = os.path.join(SAVE_FOLDER, f's{current_seed}_RF_model.joblib')\n",
    "        model_path_XGB = os.path.join(SAVE_FOLDER, f's{current_seed}_XGB_model.joblib')\n",
    "\n",
    "        joblib.dump(clsf_LR, model_path_LR)\n",
    "        joblib.dump(clsf_RF, model_path_RF)\n",
    "        joblib.dump(clsf_XGB, model_path_XGB)\n",
    "        \n",
    "        \n",
    "        ## SHAP\n",
    "        ## 1. LR\n",
    "        explainer = shap.Explainer(clsf_LR, X_train)\n",
    "        shap_values = explainer(X_test)\n",
    "        plot_shap (X_test, shap_values, my_model, 'LR', SAVE_FOLDER, current_seed)\n",
    "        \n",
    "        ## 2. RF\n",
    "        explainer = shap.TreeExplainer(clsf_RF, X_train)\n",
    "        shap_values = explainer(X_test, check_additivity=False)[:,:,1]\n",
    "        plot_shap (X_test, shap_values, my_model, 'RF', SAVE_FOLDER, current_seed)\n",
    "    \n",
    "        ## 3. XGBoost\n",
    "        explainer = shap.TreeExplainer(clsf_XGB, X_train)\n",
    "        shap_values = explainer(X_test, check_additivity=False)\n",
    "        plot_shap (X_test, shap_values, my_model, 'XGBoost', SAVE_FOLDER, current_seed)\n",
    "        \n",
    "\n",
    "        \n",
    "display(df_finale_results)\n",
    "df_finale_results.to_excel('/workspace/save_results_folder/total_result.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e48c5ca-e6cb-4be9-b695-f321723690c5",
   "metadata": {},
   "source": [
    "## Checking correlation coefficient between features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0cc623-4540-40df-8b25-91e3cf32b30b",
   "metadata": {},
   "source": [
    "### Pearson correlation coefficient for continuous variables --> SNBRs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccc18af-7031-49c4-8f59-8bbbaee69cab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "var_list = ['1_VS/Calc_Bi-1_Z', '2_PP/Calc_Bi-1_Z', '3_PC/Calc_Bi-1_Z', '4_AC/Calc_Bi-1_Z', '5_AP/Calc_Bi-1_Z',\n",
    "            '1_Lim/Calc_Bi-1_Z', '2_Exe/Calc_Bi-1_Z', '3_Sen/Calc_Bi-1_Z']\n",
    "\n",
    "results = []\n",
    "\n",
    "for var1, var2 in itertools.combinations(var_list, 2):\n",
    "    corr, p_value = stats.pearsonr(df_z[var1], df_z[var2])\n",
    "    results.append((var1, var2, corr, p_value))\n",
    "\n",
    "corr_df = pd.DataFrame(results, columns=['Variable 1', 'Variable 2', 'Pearson Correlation', 'P-Value'])\n",
    "corr_df = corr_df.sort_values(by=['Pearson Correlation'], ascending=False)\n",
    "\n",
    "display(corr_df)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e146e3cc-06c4-426c-8b5c-18836d645700",
   "metadata": {},
   "source": [
    "### Spearman rank correlation coefficient for clinical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84967f22-8ea4-4b5b-926b-283a89668e12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "var_list = ['Sex_Z', 'RT_RA_Z', 'RT_LA_Z', 'RT_RL_Z', 'RT_LL_Z', 'FT_R_Z', 'FT_L_Z', 'LA_R_Z', 'LA_L_Z', 'RG_RA_Z', 'RG_LA_Z',\n",
    "            'Symptom_Onset_Age_Z', 'Symptom_Onset_Age_60_Z', 'Gap_in_3months_Z',\n",
    "            'HY_rnd_Z', 'Tremor_total_Z', 'Brady_total_Z', 'Rigid_total_Z', \n",
    "            '1_VS/Calc_Bi-1_Z', '2_PP/Calc_Bi-1_Z', '3_PC/Calc_Bi-1_Z', '4_AC/Calc_Bi-1_Z', '5_AP/Calc_Bi-1_Z']\n",
    "\n",
    "results = []\n",
    "\n",
    "for var1, var2 in itertools.combinations(var_list, 2):\n",
    "    corr, p_value = stats.spearmanr(df_z[var1], df_z[var2])\n",
    "    results.append((var1, var2, corr, p_value))\n",
    "\n",
    "corr_df = pd.DataFrame(results, columns=['Variable 1', 'Variable 2', 'Spearman Correlation', 'P-Value'])\n",
    "corr_df = corr_df.sort_values(by=['Spearman Correlation'], ascending=False)\n",
    "\n",
    "display(corr_df)\n",
    "corr_df.to_excel('/workspace/save_results_folder/corr_df.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b134f57e-e725-4752-9633-383848c600e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a07950-8559-41b3-825e-f0554071ef3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955e0e03-63ab-42e0-8e0d-fbad6a62f05b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85288ca-edb3-4bef-9c53-8debe496b9f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7213cdc4-ca12-4554-933b-d1f69113ec5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa641e4-19ad-4897-87db-0bfc86f1f81e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
